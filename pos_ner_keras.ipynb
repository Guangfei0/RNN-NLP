{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, GRU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.optimizers import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 20000\n",
    "max_tags = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_pos(split_sequences = False):\n",
    "    if not os.path.exists('../Data/chunking'):\n",
    "        print(\"No data existed\")\n",
    "        exit()\n",
    "    \n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line in open('../Data/chunking/train.txt'):\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            r = line.split()\n",
    "            word, tag, _ = r\n",
    "            currentX.append(word)\n",
    "            currentY.append(tag)\n",
    "        \n",
    "        elif split_sequences:\n",
    "            Xtrain.append(currentX)\n",
    "            Ytrain.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "    \n",
    "    if not split_sequences:\n",
    "        Xtrain = currentX\n",
    "        Ytrain = currentY\n",
    "    \n",
    "    Xtest = []\n",
    "    Ytest = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line in open('../Data/chunking/test.txt'):\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            r = line.split()\n",
    "            word, tag, _ = r \n",
    "            currentX.append(word)\n",
    "            currentY.append(tag)\n",
    "        elif split_sequences:\n",
    "            Xtest.append(currentX)\n",
    "            Ytest.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "    \n",
    "    if not split_sequences:\n",
    "        Xtest = currentX\n",
    "        Ytest = currentY\n",
    "    \n",
    "    return Xtrain, Ytrain, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ner(split_sequences = False):\n",
    "    Xtrain = []\n",
    "    Ytrain = []\n",
    "    currentX = []\n",
    "    currentY = []\n",
    "    for line in open('ner.txt'):\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            r = line.split()\n",
    "            word, tag = r\n",
    "            word = word.lower()\n",
    "            currentX.append(word)\n",
    "            currentY.append(tag)\n",
    "        elif split_sequences:\n",
    "            Xtrain.append(currentX)\n",
    "            Ytrain.append(currentY)\n",
    "            currentX = []\n",
    "            currentY = []\n",
    "    \n",
    "    if not split_sequences:\n",
    "        Xtrain = currentX\n",
    "        Ytrain = currentY\n",
    "    \n",
    "    Xtrain, Ytrain = shuffle(Xtrain, Ytrain)\n",
    "    Ntest = int(0.3 * len(Xtrain))\n",
    "    Xtest = Xtrain[:Ntest]\n",
    "    Ytest = Ytrain[:Ntest]\n",
    "    Xtrain = Xtrain[Ntest:]\n",
    "    Ytrain = Ytrain[Ntest:]\n",
    "    \n",
    "    return Xtrain, Ytrain, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain, Ytrain, Xtest, Ytest = get_data_pos(split_sequences = True)\n",
    "Xtrain, Ytrain, Xtest, Ytest = get_data_ner(split_sequences = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 padding value has been considered into tokenizer\n",
    "tokenizer = Tokenizer(num_words = max_vocab_size)\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    "Xtrain = tokenizer.texts_to_sequences(Xtrain)\n",
    "Xtest = tokenizer.texts_to_sequences(Xtest)\n",
    "word2idx = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word2idx))\n",
    "vocab_size = min(max_vocab_size, len(word2idx) + 1) # +1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(num_words = max_tags)\n",
    "tokenizer2.fit_on_texts(Ytrain)\n",
    "Ytrain = tokenizer2.texts_to_sequences(Ytrain)\n",
    "Ytest = tokenizer2.texts_to_sequences(Ytest)\n",
    "tag2idx = tokenizer2.word_index\n",
    "print('Found %s unique tags.' % len(tag2idx))\n",
    "num_tags = min(max_tags, len(tag2idx) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = max(len(x) for x in Xtrain + Xtest)\n",
    "Xtrain = pad_sequences(Xtrain, maxlen = sequence_length)\n",
    "Ytrain = pad_sequences(Ytrain, maxlen = sequence_length)\n",
    "Xtest = pad_sequences(Xtest, maxlen = sequence_length)\n",
    "Ytest = pad_sequences(Ytest, maxlen = sequence_length)\n",
    "print('Xtrain.shape:', Xtrain.shape)\n",
    "print('Ytrain.shape:', Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_onehot = np.zeros((len(Ytrain), sequence_length, num_tags), dtype = 'float32')\n",
    "for n, sample in enumerate(Ytrain):\n",
    "    for t, tag in enumerate(sample):\n",
    "        Ytrain_onehot[n, t, tag] = 1\n",
    "\n",
    "Ytest_onehot = np.zeros((len(Ytest), sequence_length, num_tags), dtype = 'float32')\n",
    "for n, sample in enumerate(Ytest):\n",
    "    for t, tag in enumerate(sample):\n",
    "        Ytest_onehot[n, t, tag] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 32\n",
    "hidden_layer_size = 10\n",
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape = (sequence_length,))\n",
    "x = Embedding(vocab_size, embedding_dim)(input_) # x: N x T x D\n",
    "x = GRU(hidden_layer_size, return_sequences = True)(x) # N x T x M\n",
    "output = Dense(num_tags, activation = 'softmax')(x) # N x T x K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_, output)\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = Adam(lr = 1e-2),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "r = model.fit(\n",
    "    Xtrain,\n",
    "    Ytrain_onehot,\n",
    "    batch_size = batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = (Xtest, Ytest_onehot)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label = 'loss')\n",
    "plt.plot(r.history['val_loss'], label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['acc'], label = 'accuracy')\n",
    "plt.plot(r.shitory['val_acc'], label = 'val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
