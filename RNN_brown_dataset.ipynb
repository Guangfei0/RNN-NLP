{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from GRU_class.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "import import_ipynb\n",
    "from GRU_class import GRU, init_weight\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_WORDS = set([ \n",
    "   'king', 'man', 'queen', 'woman',\n",
    "   'italy', 'rome', 'france', 'paris',\n",
    "   'london', 'britain', 'england',\n",
    "])\n",
    "\n",
    "def get_sentences_with_word2idx_limit_vocab(n_vocab = 2000, keep_words = KEEP_WORDS):\n",
    "    sentences = brown.sents()\n",
    "    indexed_sentences = []\n",
    "    \n",
    "    word2idx = {'START':0, 'END':1}\n",
    "    idx2word = ['START', 'END']\n",
    "    word_idx_count = {\n",
    "        0: float('inf'),\n",
    "        1: float('inf'),\n",
    "    }\n",
    "    \n",
    "    current_idx = 2\n",
    "    for sentence in sentences:\n",
    "        indexed_sentence = []\n",
    "        for token in sentence:\n",
    "            token = token.lower()\n",
    "            if token not in word2idx:\n",
    "                idx2word.append(token)\n",
    "                word2idx[token] = current_idx\n",
    "                current_idx += 1 \n",
    "                \n",
    "            idx = word2idx[token]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "            \n",
    "            indexed_sentence.append(idx)\n",
    "        indexed_sentences.append(indexed_sentence)\n",
    "    \n",
    "    for word in keep_words:\n",
    "        word_idx_count[word2idx[word]] = float('inf')\n",
    "    \n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    \n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    word2idx_small['UNKNOWN'] = new_idx\n",
    "    unknown = new_idx\n",
    "    \n",
    "    assert('START' in word2idx_small)\n",
    "    assert('END' in word2idx_small)\n",
    "    for word in keep_words:\n",
    "        assert(word in word2idx_small)\n",
    "    \n",
    "    sentences_small = []\n",
    "    for sentence in indexed_sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "    \n",
    "    return sentences_small, word2idx_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, D, hidden_layer_sizes, V):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "    \n",
    "    def fit(self, X, learning_rate = 1e-5, mu = 0.99, epochs = 10, show_fig = True, activation = T.nnet.relu, RecurrentUnit = GRU, normalize = True):\n",
    "        D = self.D\n",
    "        V = self.V\n",
    "        N = len(X)\n",
    "        \n",
    "        We = init_weight(V, D)\n",
    "        self.hidden_layers = []\n",
    "        Mi = D\n",
    "        for Mo in self.hidden_layer_sizes:\n",
    "            ru = RecurrentUnit(Mi, Mo, activation)\n",
    "            self.hidden_layers.append(ru)\n",
    "            Mi = Mo\n",
    "        \n",
    "        Wo = init_weight(Mi, V)\n",
    "        bo = np.zeros(V)\n",
    "        \n",
    "        self.We = theano.shared(We)\n",
    "        self.Wo = theano.shared(Wo)\n",
    "        self.bo = theano.shared(bo)\n",
    "        self.params = [self.Wo, self.bo]\n",
    "        for ru in self.hidden_layers:\n",
    "            self.params += ru.params\n",
    "        \n",
    "        thX = T.ivector('X')\n",
    "        thY = T.ivector('Y')\n",
    "        \n",
    "        Z = self.We[thX] # size T x D\n",
    "        for ru in self.hidden_layers:\n",
    "            Z = ru.output(Z)\n",
    "        py_x = T.nnet.softmax(Z.dot(self.Wo) + self.bo) # T x V\n",
    "        \n",
    "        prediction = T.argmax(py_x, axis = 1)\n",
    "        self.predict_op = theano.function(\n",
    "            inputs = [thX],\n",
    "            outputs = [py_x, prediction],\n",
    "            allow_input_downcast = True,\n",
    "        )\n",
    "        \n",
    "        cost = -T.mean(T.log(py_x[T.arange(thY.shape[0]), thY]))\n",
    "        grads = T.grad(cost, self.params)\n",
    "        dparams = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "        \n",
    "        dWe = theano.shared(self.We.get_value()*0)\n",
    "        gWe = T.grad(cost, self.We)\n",
    "        dWe_update = mu*dWe - learning_rate*gWe\n",
    "        We_update = self.We + dWe_update\n",
    "        if normalize:\n",
    "            We_update /= We_update.norm(2)\n",
    "        \n",
    "        updates = [\n",
    "            (p, p + mu*dp - learning_rate*g) for p, dp, g in zip(self.params, dparams, grads)\n",
    "        ] + [\n",
    "            (dp, mu*dp - learning_rate*g) for dp, g in zip(dparams, grads)\n",
    "        ] + [\n",
    "            (self.We, We_update), (dWe, dWe_update)\n",
    "        ]\n",
    "        self.train_op = theano.function(\n",
    "            inputs = [thX, thY],\n",
    "            outputs = [cost, prediction],\n",
    "            updates = updates,\n",
    "        )\n",
    "        \n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            t0 = datetime.now()\n",
    "            X = shuffle(X)\n",
    "            n_correct = 0\n",
    "            n_total = 0\n",
    "            cost = 0\n",
    "            for j in range(N):\n",
    "                if np.random.random() < 0.01 or len(X[j]) <= 1:\n",
    "                    input_sequence = [0] + X[j]\n",
    "                    output_sequence = X[j] + [1]\n",
    "                else:\n",
    "                    input_sequence = [0] + X[j][:-1]\n",
    "                    output_sequence = X[j]\n",
    "                n_total += len(output_sequence)\n",
    "                \n",
    "                c, p = self.train_op(input_sequence, output_sequence)\n",
    "                cost +=c \n",
    "                for pj, xj in zip(p, output_sequence):\n",
    "                    if pj == xj:\n",
    "                        n_correct += 1 \n",
    "                if j % 200 == 0:\n",
    "                    sys.stdout.write(\"j/N: %d/%d, correct rate: %f\\r\" %(j, N, float(n_correct)/n_total))\n",
    "            print(\"i:\", i, \"cost:\", cost, \"correct rate:\", (float(n_correct)/n_total), \"time for epoch:\", (datetime.now()-t0))\n",
    "            costs.append(cost)\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_brown(we_file = 'word_embedding.npy', w2i_file = 'brown_word2idx.json', RecurrentUnit = GRU):\n",
    "    sentences, word2idx = get_sentences_with_word2idx_limit_vocab()\n",
    "    rnn = RNN(30, [30], len(word2idx))\n",
    "    rnn.fit(sentences, learning_rate = 1e-5, epochs = 10, show_fig = True, activation = T.nnet.relu)\n",
    "    np.save(we_file, rnn.We.get_value())\n",
    "    with open(w2i_file, 'w') as f:\n",
    "        json.dump(word2idx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_analogies(w1, w2, w3, we_file = 'word_embedding.npy', w2i_file = 'brown_word2idx.json'):\n",
    "    We = np.load(we_file)\n",
    "    with open(w2i_file) as f:\n",
    "        word2idx = json.load(f)\n",
    "    king = We[word2idx[w1]]\n",
    "    man = We[word2idx[w2]]\n",
    "    woman = We[word2idx[w3]]\n",
    "    v0 = king - man + woman\n",
    "    \n",
    "    def dist1(a, b):\n",
    "        return np.linalg.norm(a-b)\n",
    "    def dist2(a, b):\n",
    "        return 1 - a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    for dist, name in [(dist1, 'Euclidean'), (dist2, 'cosine')]:\n",
    "        min_dist = float('inf')\n",
    "        best_word = ''\n",
    "        for word, idx in word2idx.items():\n",
    "            v1 = We[idx]\n",
    "            d = dist(v0, v1)\n",
    "            if d < min_dist:\n",
    "                min_dist = d\n",
    "                best_word = word\n",
    "        print(\"best match by\", name, \"distance:\", best_word)\n",
    "        print(w1, \"-\", w2, \"=\", best_word, \"-\", w3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    we = 'gru_word_embedding2.npy'\n",
    "    w2i = 'gru_brown_word2idx.json'\n",
    "    train_brown(we, w2i, RecurrentUnit = GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best match by Euclidean distance: services\n",
      "france - paris = services - london\n",
      "best match by cosine distance: london\n",
      "france - paris = london - london\n"
     ]
    }
   ],
   "source": [
    "find_analogies('france', 'paris', 'london', we, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best match by Euclidean distance: direct\n",
      "king - man = direct - woman\n",
      "best match by cosine distance: low\n",
      "king - man = low - woman\n"
     ]
    }
   ],
   "source": [
    "find_analogies('king', 'man', 'woman', we, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
